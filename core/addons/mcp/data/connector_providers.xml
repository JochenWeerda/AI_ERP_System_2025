<?xml version="1.0" encoding="utf-8"?>
<odoo>
    <data noupdate="0">
        <!-- Provider-Konfigurationen für AI-Dienste -->
        <record id="mcp_provider_anthropic" model="mcp.provider">
            <field name="name">Anthropic Claude</field>
            <field name="code">anthropic</field>
            <field name="description">Anthropic Claude 3 Modelle für fortschrittliche KI-Assistenz</field>
            <field name="provider_type">external</field>
            <field name="api_endpoint">https://api.anthropic.com/v1/messages</field>
            <field name="api_version">2023-06-01</field>
            <field name="active">true</field>
            <field name="connector_class">AnthropicConnector</field>
            <field name="default_parameters">{
                "temperature": 0.7,
                "max_tokens": 1024,
                "top_p": 0.95,
                "anthropic_version": "2023-06-01"
            }</field>
        </record>
        
        <record id="mcp_provider_openai" model="mcp.provider">
            <field name="name">OpenAI</field>
            <field name="code">openai</field>
            <field name="description">OpenAI GPT-4 und GPT-3.5 Modelle</field>
            <field name="provider_type">external</field>
            <field name="api_endpoint">https://api.openai.com/v1/chat/completions</field>
            <field name="api_version">v1</field>
            <field name="active">true</field>
            <field name="connector_class">OpenAIConnector</field>
            <field name="default_parameters">{
                "temperature": 0.7,
                "max_tokens": 1024,
                "top_p": 0.95,
                "frequency_penalty": 0,
                "presence_penalty": 0
            }</field>
        </record>
        
        <record id="mcp_provider_azure_openai" model="mcp.provider">
            <field name="name">Azure OpenAI</field>
            <field name="code">azure_openai</field>
            <field name="description">OpenAI-Modelle gehostet auf Microsoft Azure</field>
            <field name="provider_type">external</field>
            <field name="api_endpoint">{AZURE_OPENAI_ENDPOINT}</field>
            <field name="api_version">2023-05-15</field>
            <field name="active">true</field>
            <field name="connector_class">AzureOpenAIConnector</field>
            <field name="default_parameters">{
                "temperature": 0.7,
                "max_tokens": 1024,
                "top_p": 0.95,
                "frequency_penalty": 0,
                "presence_penalty": 0,
                "deployment_name": "{DEPLOYMENT_NAME}"
            }</field>
        </record>
        
        <record id="mcp_provider_ollama" model="mcp.provider">
            <field name="name">Ollama (Lokal)</field>
            <field name="code">ollama</field>
            <field name="description">Lokale KI-Modelle über Ollama</field>
            <field name="provider_type">local</field>
            <field name="api_endpoint">http://localhost:11434/api/generate</field>
            <field name="api_version">v1</field>
            <field name="active">true</field>
            <field name="connector_class">OllamaConnector</field>
            <field name="default_parameters">{
                "temperature": 0.7,
                "num_predict": 1024,
                "top_p": 0.95,
                "model": "llama3"
            }</field>
        </record>
        
        <record id="mcp_provider_llama_cpp" model="mcp.provider">
            <field name="name">Llama.cpp (Lokal)</field>
            <field name="code">llama_cpp</field>
            <field name="description">Lokal ausgeführte Llama Modelle über llama.cpp</field>
            <field name="provider_type">local</field>
            <field name="api_endpoint">http://localhost:8080/v1/chat/completions</field>
            <field name="api_version">v1</field>
            <field name="active">true</field>
            <field name="connector_class">LlamaCppConnector</field>
            <field name="default_parameters">{
                "temperature": 0.7,
                "max_tokens": 1024,
                "top_p": 0.95,
                "repeat_penalty": 1.1
            }</field>
        </record>
        
        <!-- Modellkonfigurationen -->
        <record id="mcp_model_claude_3_opus" model="mcp.model">
            <field name="name">Claude 3 Opus</field>
            <field name="code">claude-3-opus-20240229</field>
            <field name="description">Anthropic's leistungsstärkstes Modell für komplexe Aufgaben</field>
            <field name="provider_id" ref="mcp_provider_anthropic"/>
            <field name="model_type">text</field>
            <field name="max_tokens">200000</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "claude-3-opus-20240229",
                "temperature": 0.5,
                "max_tokens": 4096,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_claude_3_sonnet" model="mcp.model">
            <field name="name">Claude 3 Sonnet</field>
            <field name="code">claude-3-sonnet-20240229</field>
            <field name="description">Anthropic's ausgewogenes Modell für die meisten Anwendungsfälle</field>
            <field name="provider_id" ref="mcp_provider_anthropic"/>
            <field name="model_type">text</field>
            <field name="max_tokens">200000</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "claude-3-sonnet-20240229",
                "temperature": 0.7,
                "max_tokens": 4096,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_claude_3_haiku" model="mcp.model">
            <field name="name">Claude 3 Haiku</field>
            <field name="code">claude-3-haiku-20240307</field>
            <field name="description">Anthropic's schnellstes und kostengünstigstes Modell</field>
            <field name="provider_id" ref="mcp_provider_anthropic"/>
            <field name="model_type">text</field>
            <field name="max_tokens">200000</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "claude-3-haiku-20240307",
                "temperature": 0.7,
                "max_tokens": 4096,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_gpt_4_turbo" model="mcp.model">
            <field name="name">GPT-4 Turbo</field>
            <field name="code">gpt-4-turbo-preview</field>
            <field name="description">OpenAI's fortschrittlichstes Modell mit aktuellem Wissen</field>
            <field name="provider_id" ref="mcp_provider_openai"/>
            <field name="model_type">text</field>
            <field name="max_tokens">128000</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "gpt-4-turbo-preview",
                "temperature": 0.7,
                "max_tokens": 4096,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_gpt_4" model="mcp.model">
            <field name="name">GPT-4</field>
            <field name="code">gpt-4</field>
            <field name="description">OpenAI's leistungsstarkes Modell für komplexe Aufgaben</field>
            <field name="provider_id" ref="mcp_provider_openai"/>
            <field name="model_type">text</field>
            <field name="max_tokens">8192</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 2048,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_gpt_3_5_turbo" model="mcp.model">
            <field name="name">GPT-3.5 Turbo</field>
            <field name="code">gpt-3.5-turbo</field>
            <field name="description">OpenAI's ausgewogenes und schnelles Modell</field>
            <field name="provider_id" ref="mcp_provider_openai"/>
            <field name="model_type">text</field>
            <field name="max_tokens">16384</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "gpt-3.5-turbo",
                "temperature": 0.7,
                "max_tokens": 2048,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_azure_gpt_4" model="mcp.model">
            <field name="name">Azure GPT-4</field>
            <field name="code">azure-gpt-4</field>
            <field name="description">OpenAI GPT-4 gehostet auf Microsoft Azure</field>
            <field name="provider_id" ref="mcp_provider_azure_openai"/>
            <field name="model_type">text</field>
            <field name="max_tokens">8192</field>
            <field name="active">true</field>
            <field name="parameters">{
                "deployment_name": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 2048,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_ollama_llama3" model="mcp.model">
            <field name="name">Llama 3 (Ollama)</field>
            <field name="code">llama3</field>
            <field name="description">Meta's Llama 3 lokal mit Ollama ausgeführt</field>
            <field name="provider_id" ref="mcp_provider_ollama"/>
            <field name="model_type">text</field>
            <field name="max_tokens">4096</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "llama3",
                "temperature": 0.7,
                "num_predict": 2048,
                "top_p": 0.95
            }</field>
        </record>
        
        <record id="mcp_model_llama_cpp_mistral" model="mcp.model">
            <field name="name">Mistral (llama.cpp)</field>
            <field name="code">mistral</field>
            <field name="description">Mistral 7B lokal mit llama.cpp ausgeführt</field>
            <field name="provider_id" ref="mcp_provider_llama_cpp"/>
            <field name="model_type">text</field>
            <field name="max_tokens">4096</field>
            <field name="active">true</field>
            <field name="parameters">{
                "model": "mistral",
                "temperature": 0.7,
                "max_tokens": 2048,
                "top_p": 0.95,
                "repeat_penalty": 1.1
            }</field>
        </record>
    </data>
</odoo> 